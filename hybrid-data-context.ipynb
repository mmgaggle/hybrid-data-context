{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Data Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intent of this notebook is to provide examples of how data scientists can use object storage, and more specifically, Ceph object storage, much in the same way they are accoustomed to interacting with Amazon Simple Storage Service (S3). This is made possible because Ceph's object storage gateway offers excellent fidelity with the modalities of Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Boto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boto is an integrated interface to current and future infrastructural services offered by Amazon Web Services. Amoung the services it provides interfaces for is Amazon S3. For lightweight analysis of data using python tools like numpy or pandas, it is handy to interact with data stored in object storage using pure python. This is where Boto shines. The base-notebook from [radanalyticsio](https://radanalytics.io) doesn't include Boto, but you can install it from the comfort of a notebook using the conda install command below. If you find yourself using Boto frequently, it might be worth modifying [base-notebook](https://github.com/radanalyticsio/base-notebook) and building a custom notebook image that includes Boto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install --yes --quiet --prefix {sys.prefix} boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3','us-east-1', endpoint_url= os.environ['RGW_API_ENDPOINT'],\n",
    "                       aws_access_key_id = os.environ['RGW_USER_USER_KEY'],\n",
    "                       aws_secret_access_key = os.environ['RGW_USER_SECRET_KEY'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a bucket, uploading and object (put), and listing the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3.create_bucket(Bucket='ceph-bucket')\n",
    "s3.put_object(Bucket='ceph-bucket',Key='object',Body='data')\n",
    "for key in s3.list_objects(Bucket='ceph-bucket')['Contents']:\n",
    "    print(key['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running an application "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: explain local vs oshinko clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[3]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", os.environ['RGW_API_ENDPOINT'])\n",
    "hadoopConf.set(\"fs.s3a.access.key\", os.environ['RGW_USER_USER_KEY'])\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", os.environ['RGW_USER_SECRET_KEY'])\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "spark.range(100, numPartitions=100).rdd.map(lambda x: socket.gethostname()).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df0 = spark.read.text(\"s3a://ceph-bucket/object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Public Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of Hadoop 2.8, S3A supports per bucket configuration. This is very powerful. It allows us to have a distinct S3A configuration, with a different endpoint and different set of credentials. With this I can use a single Spark context to read a parquet file from a bucket in the public cloud (Amazon S3) into a data frame, then turn around and write that dataframe as a parquet file into a bucket that exists in the Ceph Nano service running in Minishift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.bucket.radanalytics-data.endpoint\", \"s3.amazonaws.com\")\n",
    "hadoopConf.set(\"fs.s3a.bucket.radanalytics-data.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.parquet(\"s3a://radanalytics-data/wikieod.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.write.parquet(\"s3a://ceph-bucket/wikieod.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.bucket.bd-dist.endpoint\", \"s3.amazonaws.com\")\n",
    "hadoopConf.set(\"fs.s3a.bucket.bd-dist.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply read tab separated data from a bucket in Amazon S3 and write it back out to a bucket in our Ceph Nano service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(\"s3a://bd-dist/trip_report.tsv\",sep=\"\\t\").write.csv(\"s3a://ceph-bucket/trip_report.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract tab separated data from a bucket in Amazon S3 and reserialize it as parquet as we're writing it out to a bucket in our Ceph Nano service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark.read.csv(\"s3a://bd-dist/trip_report.tsv\",sep=\"\\t\").write.parquet(\"s3a://ceph-bucket/trip_report.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.schema.jsonValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.registerTempTable(\"wikieod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rht = spark.sql(\"select * from wikieod where ticker = 'RHT'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rht.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "_ = sns.tsplot(rht.sort(rht.date).toPandas().close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Local TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by installing TensorFlow, along with several other machine learning libraries that we will need for our machine learning example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sklearn tensorflow keras pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Access the data using Spark__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already established the Spark Context above, so we can use it to load the trip report from the TSV object we wrote into our Ceph Nano service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feedbackFile = spark.read.csv(\"s3a://ceph-bucket2/trip_report.tsv\",sep=\"\\t\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can load the trip report from the original sample TSV object in Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feedbackFile = spark.read.csv(\"s3a://bd-dist/trip_report.tsv\",sep=\"\\t\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert the data to a Pandas data frame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = feedbackFile.toPandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Types of trip outcomes by field representative__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(sum(map(ord, \"categorical\")))\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "outcome_dict = {'Successful':0,'Partial Success':1,'Unsuccessful':2 }\n",
    "\n",
    "df_vis = df[['Your Name', 'Outcome']]\n",
    "df_vis['outcome_numeric'] = df_vis['Outcome'].apply(lambda a:outcome_dict[a])\n",
    "\n",
    "\n",
    "\n",
    "outcome_cross_table = pd.crosstab(index=df_vis[\"Your Name\"], \n",
    "                          columns=df_vis[\"Outcome\"])\n",
    "\n",
    "\n",
    "outcome_cross_table.plot(kind=\"bar\", \n",
    "                 figsize=(16,12),\n",
    "                 stacked=True,fontsize=12)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Types of outcomes by event type__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type_cross_table = pd.crosstab(index=df[\"Primary Audience Engaged\"], \n",
    "                          columns=df[\"Outcome\"])\n",
    "\n",
    "event_type_cross_table.plot(kind=\"bar\", \n",
    "                 figsize=(16,12),\n",
    "                 stacked=True,fontsize=12)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now convert \"Highlights\" data to prepare for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Highlights'] = df['Highlights'].astype(str)\n",
    "\n",
    "df[['Highlights','Outcome']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcome = df[['Highlights','Outcome']]\n",
    "\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "grouped_highlights = pd.DataFrame(df_outcome.groupby('Outcome')['Highlights'].apply(lambda x: \"%s\" % ' '.join(x)))\n",
    "\n",
    "grouped_highlights['Outcome'] = list(grouped_highlights.index.get_values())\n",
    "grouped_highlights.reset_index(drop=True, inplace=True)\n",
    "\n",
    "grouped_highlights['Highlights'] = grouped_highlights['Highlights'].astype(str)\n",
    "\n",
    "df['Highlights'] = df['Highlights'].apply(lambda a: a.lower())\n",
    "\n",
    "df_success = df[df['Outcome'] == 'Successful']\n",
    "df_unsuccess = df[df['Outcome'] == 'Unsuccessful']\n",
    "df_part_success = df[df['Outcome'] == 'Partial Success']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import additional Machine Learning libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Separating train and test data. Taking successful and unsuccessful separately__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failure = df_part_success.append(df_unsuccess, ignore_index= True)\n",
    "\n",
    "df_failure['Outcome'] = 'Unsuccessful'\n",
    "\n",
    "test_hold_out = 0.1\n",
    "\n",
    "#### Success\n",
    "\n",
    "train = df_success[ : -int(test_hold_out * len(df_success))]\n",
    "test = df_success[-int(test_hold_out * len(df_success)) : ]\n",
    "\n",
    "#### Failure\n",
    "\n",
    "train = train.append(df_failure[ : -int(test_hold_out * len(df_failure))])\n",
    "test = test.append(df_failure[-int(test_hold_out * len(df_failure)) : ])\n",
    "\n",
    "\n",
    "train = train.sample(frac = 1)\n",
    "train['type'] = \"Train\"\n",
    "test['type'] = \"Test\"\n",
    "\n",
    "train = train.append(test)\n",
    "\n",
    "train.reset_index(drop=True,inplace=True)\n",
    "\n",
    "Y = pd.get_dummies(train['Outcome']).values\n",
    "\n",
    "test_index_list = list(train[train['type'] == 'Test'].index)\n",
    "\n",
    "test_index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the HIGHLIGHTS field for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__max_features__ = Vocabulary size, its a hyper parameter\n",
    "\n",
    "*Tokenizer creates vectors from text, mainly works like a dictionary id in total vocabulary, returns list of integers, where every integer acts like an index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 10000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(train['Highlights'].values)\n",
    "X_highlights = tokenizer.texts_to_sequences(train['Highlights'].values)\n",
    "X_highlights = pad_sequences(X_highlights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating the network layer by layer__\n",
    "\n",
    "First layer is word embedding layer, second layer is LSTM based RNN, and third layer is Softmax activation layer, due to categorical outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X_highlights.shape[1], dropout=0.05))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.1, dropout_W=0.1))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Separating train and test data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_highlights_train = X_highlights[0:test_index_list[0]]\n",
    "Y_highlights_train = Y[0:test_index_list[0]]\n",
    "\n",
    "X_highlights_test = X_highlights[test_index_list[0]:]\n",
    "Y_highlights_test = Y[test_index_list[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Running the model__\n",
    "\n",
    "Batch size and number of epoch can be changed as optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "model.fit(X_highlights_train, Y_highlights_train, epochs = 10, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Printing test data accuracy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score,accuracy = model.evaluate(X_highlights_test, Y_highlights_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"accuracy: %.2f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model, tokenizer and feature dimension and store them in Ceph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"./model\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('./tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "feature_dimension = X_highlights_train.shape[1]\n",
    "with open('./feature_dimension.pickle', 'wb') as handle:\n",
    "    pickle.dump(feature_dimension, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "#Create S3 session for writing manifest file\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id = os.environ['RGW_USER_USER_KEY'],\n",
    "    aws_secret_access_key = os.environ['RGW_USER_SECRET_KEY']\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3', endpoint_url=os.environ['RGW_API_ENDPOINT'], verify=False)\n",
    "\n",
    "# Upload the model to S3\n",
    "s3.meta.client.upload_file('./model', 'ceph-bucket', 'models/trip_report_model')\n",
    "\n",
    "# Upload the tokenizer to S3\n",
    "s3.meta.client.upload_file('./tokenizer.pickle', 'ceph-bucket', 'models/trip_report_tokenizer.pickle')\n",
    "\n",
    "# Upload the feature dimension to S3\n",
    "s3.meta.client.upload_file('./feature_dimension.pickle', 'ceph-bucket', 'models/trip_report_feature_dimension.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been saved to s3 as binary objects and can be viewed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
